As a machine learning researcher, I see deep urgency in this year’s Open Access Week theme. Open access, structural equity, and inclusion must be made explicit goals in artificial intelligence (AI) research. The demographics of the AI community do not reflect societal diversity. This bakes [systemic biases](https://www.technologyreview.com/2020/07/17/1005396/predictive-policing-algorithms-racist-dismantled-machine-learning-bias-criminal-justice/) into our AI algorithms, inspiring the hard work of groups like the [Algorithmic Justice League](https://www.npr.org/sections/codeswitch/2020/02/08/770174171/when-bias-is-coded-into-our-technology). But even if we know *who* is writing the algorithms that affect our lives, we often don’t know *how* these predictive systems make their decisions. A recent [response](https://www.nature.com/articles/s41586-020-2766-y) to a Google Health [closed source tool](https://www.nature.com/articles/s41586-019-1799-6) for breast cancer screening argues that open source code and release of training data are necessary for computational reproducibility in AI, and that failing to release code and data undermines the scientific value of a publication. Ironically, however, this well-worded argument lies behind a paywall. Competing views on closed access AI publishing are captured in the 2018 [boycott](https://openaccess.engineering.oregonstate.edu) of *Nature Machine Intelligence*, its [coverage](https://www.sciencemag.org/news/2018/05/why-are-ai-researchers-boycotting-new-nature-journal-and-shunning-others) in the scientific media, and the journal’s [rebuttal](https://www.nature.com/articles/s42256-020-0144-y). 

To [quote](https://www.ajl.org) the Algorithmic Justice League, "Technology should serve all of us. Not just the priviledged few." Increasing democratic access to information through open access publishing is an important step toward this goal.
