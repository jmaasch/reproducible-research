As a machine learning researcher, I see deep urgency in this year’s Open Access Week theme. Open access, structural equity, and inclusion must be made explicit goals in artificial intelligence (AI) research. The demographics of the AI community do not reflect societal diversity. This bakes [systemic biases](https://www.technologyreview.com/2020/07/17/1005396/predictive-policing-algorithms-racist-dismantled-machine-learning-bias-criminal-justice/) into our AI algorithms, inspiring the hard work of groups like the [Algorithmic Justice League](https://www.npr.org/sections/codeswitch/2020/02/08/770174171/when-bias-is-coded-into-our-technology). But even if we know *who* is writing the algorithms that affect our lives, we often don’t know *how* these predictive systems make their decisions. A recent [response](https://www.nature.com/articles/s41586-020-2766-y) to a Google Health [closed source breast cancer screening tool](https://www.nature.com/articles/s41586-019-1799-6) argues that open source code and release of training data are necessary for computational reproducibility in AI research. Ironically, however, this well-worded argument lies behind a paywall. Competing views on the enclosure of AI are captured in the 2018 [boycott](https://openaccess.engineering.oregonstate.edu) of *Nature Machine Intelligence*, its [coverage](https://www.sciencemag.org/news/2018/05/why-are-ai-researchers-boycotting-new-nature-journal-and-shunning-others) in the scientific media, and the journal’s [rebuttal](https://www.nature.com/articles/s42256-020-0144-y). 

