As a machine learning researcher, this year’s Open Access Week theme resonates. Open access, structural equity, and inclusion should be explicit goals in artificial intelligence (AI) research. To quote the [Algorithmic Justice League](https://www.ajl.org), "Technology should serve all of us. Not just the priviledged few." However, the demographics of the AI community do not reflect societal diversity, and this can allow algorithms to reinforce harmful [systemic biases](https://www.technologyreview.com/2020/07/17/1005396/predictive-policing-algorithms-racist-dismantled-machine-learning-bias-criminal-justice/). But even if we know *who* is writing the algorithms that affect our lives, we often don’t know *how* these predictive systems make their decisions. A recent [response](https://www.nature.com/articles/s41586-020-2766-y) to a Google Health [closed source tool](https://www.nature.com/articles/s41586-019-1799-6) for breast cancer screening argues that failing to release code and training data undermines the scientific value, transparency, and reproducibility of AI publications. Ironically, however, this well-worded argument lies behind a paywall. Competing views on closed access AI publishing are captured in the 2018 [boycott](https://openaccess.engineering.oregonstate.edu) of *Nature Machine Intelligence*, its [coverage](https://www.sciencemag.org/news/2018/05/why-are-ai-researchers-boycotting-new-nature-journal-and-shunning-others) in the scientific media, and the journal’s [rebuttal](https://www.nature.com/articles/s42256-020-0144-y). Whether you stand by [Plan S](https://www.coalition-s.org) or not, open conversation around the ethics of access and transparency is an important step toward safe, equitable, and inclusive AI.
